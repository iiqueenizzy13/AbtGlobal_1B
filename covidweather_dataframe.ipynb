{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\\n\",\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the COVID dataset\n",
    "filename1 = os.path.join(os.getcwd(), \"weekly_covid_county_level_US.csv\")\n",
    "df1 = pd.read_csv(filename1, header=0)\n",
    "\n",
    "# Get the weather dataset\n",
    "filename2 = os.path.join(os.getcwd(), \"weekly_tavg_county_cont_US.csv\")\n",
    "df2 = pd.read_csv(filename2, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename county_name to county in weekly_covid_county_level_US.csv\n",
    "df1.rename(columns={'county_name': 'County'}, inplace=True)\n",
    "df1.rename(columns={'report_date': 'Week'}, inplace=True)\n",
    "df1.rename(columns={'state_name': 'State'}, inplace=True)\n",
    "\n",
    "# Sort df1 by County to match with df2\n",
    "sorted_df1 = df1.sort_values(by=['State','County', 'Week'])\n",
    "#display(sorted_df1)\n",
    "\n",
    "# Sort df2 by County to match with df1\n",
    "sorted_df2 = df2.sort_values(by=['State','County', 'Week'])\n",
    "    \n",
    "#Drop 10/06 and 10/13 and 05/28/23 - 06/06/24 to match with when COVID data ends from df2 \n",
    "values_todrop = ['2022-10-06', '2022-10-13', '2023-05-18', '2023-05-25', '2023-06-01', '2023-06-08', '2023-06-15', '2023-06-22', '2023-06-29', '2023-07-06',\n",
    "'2023-07-13', '2023-07-20', '2023-07-27', '2023-08-03', '2023-08-10', '2023-08-17', '2023-08-24', '2023-08-31', '2023-09-07', '2023-09-14', '2023-09-21',\n",
    "'2023-09-28', '2023-10-05', '2023-10-12', '2023-10-19', '2023-10-26', '2023-11-02', '2023-11-09', '2023-11-16', '2023-11-23', '2023-11-30', '2023-12-07',\n",
    "'2023-12-14', '2023-12-21', '2023-12-28', '2024-01-04', '2024-01-11', '2024-01-18', '2024-01-25', '2024-02-01', '2024-02-08', '2024-02-15', '2024-02-22',\n",
    "'2024-02-29', '2024-03-07', '2024-03-14', '2024-03-21', '2024-03-28', '2024-04-04', '2024-04-11', '2024-04-18', '2024-04-25', '2024-05-02', '2024-05-09',\n",
    "'2024-05-16', '2024-05-23', '2024-05-30', '2024-06-06']\n",
    "\n",
    "sorted_df2 = sorted_df2[~sorted_df2['Week'].isin(values_todrop)]\n",
    "\n",
    "#dictionary of state names to map to state abbr\n",
    "state_abbrev_to_name = {'AL': 'Alabama','AK': 'Alaska','AZ': 'Arizona','AR': 'Arkansas','CA': 'California','CO': 'Colorado','CT': 'Connecticut','DE': 'Delaware',\n",
    "'FL': 'Florida','GA': 'Georgia','HI': 'Hawaii','ID': 'Idaho','IL': 'Illinois','IN': 'Indiana','IA': 'Iowa','KS': 'Kansas','KY': 'Kentucky','LA': 'Louisiana',\n",
    "'ME': 'Maine','MD': 'Maryland','MA': 'Massachusetts','MI': 'Michigan','MN': 'Minnesota','MS': 'Mississippi','MO': 'Missouri','MT': 'Montana','NE': 'Nebraska',\n",
    "'NV': 'Nevada','NH': 'New Hampshire','NJ': 'New Jersey','NM': 'New Mexico','NY': 'New York','NC': 'North Carolina','ND': 'North Dakota','OH': 'Ohio','OK': 'Oklahoma',\n",
    "'OR': 'Oregon','PA': 'Pennsylvania','RI': 'Rhode Island','SC': 'South Carolina','SD': 'South Dakota','TN': 'Tennessee','TX': 'Texas','UT': 'Utah','VT': 'Vermont',\n",
    "'VA': 'Virginia','WA': 'Washington','WV': 'West Virginia','WI': 'Wisconsin','WY': 'Wyoming','DC': 'District of Columbia','PR': 'Puerto Rico'}\n",
    "\n",
    "sorted_df2['State'] = sorted_df2['State'].map(state_abbrev_to_name)\n",
    "#output both sorted dataframes into csv files\n",
    "sorted_df1.to_csv('sortedcovid.csv', index=False)\n",
    "sorted_df2.to_csv('sortedweather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State                                                     0\n",
      "County                                                    0\n",
      "Week                                                      0\n",
      "cases_per_100K_7_day_count_change                         0\n",
      "percent_test_results_reported_positive_last_7_days    20450\n",
      "community_transmission_level                            495\n",
      "Temperature                                               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the two datasets into a new dataset on the county column\n",
    "# The new df is sorted by the County column\n",
    "df = pd.merge(sorted_df1, sorted_df2, on=['State', 'County', 'Week'], how='inner')\n",
    "   \n",
    "# Drop FIPS, fips_code and state_abbr columns since they are unnecessary and repeating\n",
    "df = df.drop(columns=['FIPS', 'fips_code', 'state_abbr'])\n",
    "\n",
    "#print(df.head(130))\n",
    "#display(df)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State                                                     0\n",
      "County                                                    0\n",
      "Week                                                      0\n",
      "cases_per_100K_7_day_count_change                         0\n",
      "percent_test_results_reported_positive_last_7_days    20450\n",
      "community_transmission_level                            495\n",
      "Avg Temp                                                  0\n",
      "Temp Flux                                              1840\n",
      "dtype: int64\n",
      "State                                0\n",
      "County                               0\n",
      "Week                                 0\n",
      "cases_per_100K_7_day_count_change    0\n",
      "Avg Temp                             0\n",
      "Temp Flux                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#rename Temperature column to Avg Temp\n",
    "df.rename(columns={'Temperature': 'Avg Temp'}, inplace=True)\n",
    "\n",
    "#reorder dataframe columns\n",
    "order = ['State', 'County', 'Week', 'cases_per_100K_7_day_count_change','percent_test_results_reported_positive_last_7_days','community_transmission_level', 'Avg Temp']\n",
    "df = df[order]\n",
    "\n",
    "#make Temp Flux column/feature\n",
    "df['Temp Flux'] = df.groupby('County')['Avg Temp'].diff()\n",
    "\n",
    "#double check null values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "#dropped percent_test_results_reported_positive_last_7_days and community_transmission_level\n",
    "#because the columns have high null values and are not related to our ML problem\n",
    "df = df.drop(columns=['percent_test_results_reported_positive_last_7_days', 'community_transmission_level'])\\\n",
    "\n",
    "#handle NaN values for the first date\n",
    "#This occurs because temp flux = curr week - prev week, and the first week(2022-10-20) doesn't have a previous week\n",
    "#solution: Only NaN values are for 2022-10-20, so we are using the county's temp flux for 2022-10-27 to replace the NaN\n",
    "temp_flux_nulls = (df['Week'] == '2022-10-20') & (df['Temp Flux'].isna())\n",
    "df.loc[temp_flux_nulls, 'Temp Flux'] = df.groupby('County')['Temp Flux'].shift(-1)\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.to_csv('final_dataset.csv', index=False)\n",
    "#display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
